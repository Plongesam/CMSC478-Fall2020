{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>CMSC 478: Machine Learning</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Support Vector Machines (SVM)</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Support Vector Machines - SVM</center></h1>\n",
    "\n",
    "<center><img src=\"img/svm-title.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [4]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Binary Classification</center></h1>\n",
    "\n",
    "<center><img src=\"img/binary-classification.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Linear Separability</center></h1>\n",
    "\n",
    "<center><img src=\"img/linear-separability.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Linear Classifier</center></h1>\n",
    "\n",
    "<center><img src=\"img/linear-classifier.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>N-Dimensional Linear Classifiers - Hyperplane</center></h1>\n",
    "\n",
    "<center><img src=\"img/hyperplane.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Perceptron Algorithm</center></h1>\n",
    "\n",
    "<center><img src=\"img/perceptron-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Perceptron Algorithm</center></h1>\n",
    "\n",
    "<center><img src=\"img/perceptron-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Finding the Best Hyperplane</center></h1>\n",
    "\n",
    "<center><img src=\"img/best-w.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Finding the Best Hyperplane</center></h1>\n",
    "\n",
    "<center><img src=\"img/best-w-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Support Vector Machines - SVM</center></h1>\n",
    "\n",
    "<center><img src=\"img/svm-margin.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM - Hard Margin vs Soft Margin Classification</center></h1>\n",
    "\n",
    "- If we strictly impose that all instances must be off the margin and on the correct side, this is called <font color=\"blue\"><b>hard margin</b></font> classification.\n",
    "\n",
    "\n",
    "- There are two main issues with hard margin classification:\n",
    "    - First, it only works if the data is linearly separable.\n",
    "    - Second, it is sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The objective is to find a good balance between keeping the margin as large as possible and limiting the margin violations (i.e., instances that end up in the middle of the street or even on the wrong side). This is called <font color=\"blue\"><b>soft margin</b></font> classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM - Math Notations and Equations</center></h1>\n",
    "\n",
    "- The smaller the weight vector <b>w</b> the larger the margin.\n",
    "\n",
    "**Equation 5-2: Linear SVM classifier prediction**\n",
    "\n",
    "$\n",
    "\\hat{y} = \\begin{cases}\n",
    " 0 & \\text{if } \\mathbf{w}^T \\mathbf{x} + b < 0, \\\\\n",
    " 1 & \\text{if } \\mathbf{w}^T \\mathbf{x} + b \\geq 0\n",
    "\\end{cases}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM - Hard Margin Linear SVM</center></h1>\n",
    "\n",
    "- If we define $t^{(i)} = -1$ for negative instances  (if $y^{(i)} = 0$) and $t^{(i)} = +1$ for positive instances (if $y^{(i)} = 1$), then we can express the optimization constraint for all instances as the following:\n",
    "\n",
    "\n",
    "**Equation 5-3: Hard margin linear SVM classifier objective**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "&\\underset{\\mathbf{w}, b}{\\operatorname{minimize}}\\quad{\\frac{1}{2}\\mathbf{w}^T \\mathbf{w}} \\\\\n",
    "&\\text{subject to} \\quad t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\ge 1 \\quad \\text{for } i = 1, 2, \\dots, m\n",
    "\\end{split}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM - Soft Margin Linear SVM</center></h1>\n",
    "\n",
    "- To get the soft margin objective, we need to introduce a slack variable ζ(i) ≥ 0\n",
    "    - For each instance: ζ(i) measures how much the i-th instance is allowed to violate the margin.\n",
    "\n",
    "\n",
    "- We now have two conflicting objectives: make the slack variables as small as possible to reduce the margin violations, and make $\\quad{\\frac{1}{2}\\mathbf{w}^T \\mathbf{w}} $ as small as possible to increase the margin. This is where the <font color = \"blue\"><b>C hyperparameter</b></font> comes in: it allows us to define the trade‐off between these two objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Soft Margin SVM - Slack Variables</center></h1>\n",
    "\n",
    "<center><img src=\"img/slack-variable.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Soft Margin SVM - Slack Variables</center></h1>\n",
    "\n",
    "**Equation 5-4: Soft margin linear SVM classifier objective**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "&\\underset{\\mathbf{w}, b, \\mathbf{\\zeta}}{\\operatorname{minimize}}\\quad{\\dfrac{1}{2}\\mathbf{w}^T \\mathbf{w} + C \\sum\\limits_{i=1}^m{\\zeta^{(i)}}}\\\\\n",
    "&\\text{subject to} \\quad t^{(i)}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\ge 1 - \\zeta^{(i)} \\quad \\text{and} \\quad \\zeta^{(i)} \\ge 0 \\quad \\text{for } i = 1, 2, \\dots, m\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hyperparameter C - Bias-Variance Trade-Off</center></h1>\n",
    "\n",
    "- C determines the width of the margin and defines the trade‐off between the two objectives of SVM optimization constraints.\n",
    "\n",
    "\n",
    "- The larger the C, the narrower the margin, the fewer margin violations, (probably) the less generalized model, the more prone to overfitting,  <font color='blue'>the lower the bias, the higher the variance</font>.\n",
    "\n",
    "\n",
    "- The smaller the C, the wider the margin, the more margin violations, (probably) the more generalized model, the more prone to underfitting, <font color='blue'>the higher the bias, the lower the variance</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hyperparameter C - Infinity</center></h1>\n",
    "\n",
    "<center><img src=\"img/C-infinity.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hyperparameter C - Small Values</center></h1>\n",
    "\n",
    "<center><img src=\"img/C-small.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Optimization</center></h1>\n",
    "\n",
    "<center><img src=\"img/optimization.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Cost Function</center></h1>\n",
    "\n",
    "<center><img src=\"img/loss.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1><center>Quadratic Programming</center></h1>\n",
    "\n",
    "**Equation 5-5: Quadratic Programming problem**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\underset{\\mathbf{p}}{\\text{Minimize}} \\quad & \\dfrac{1}{2} \\mathbf{p}^T \\mathbf{H} \\mathbf{p} \\quad + \\quad \\mathbf{f}^T \\mathbf{p}  \\\\\n",
    "\\text{subject to} \\quad & \\mathbf{A} \\mathbf{p} \\le \\mathbf{b} \\\\\n",
    "\\text{where } &\n",
    "\\begin{cases}\n",
    "  \\mathbf{p} & \\text{ is an }n_p\\text{-dimensional vector (} n_p = \\text{number of parameters),}\\\\\n",
    "  \\mathbf{H} & \\text{ is an }n_p \\times n_p \\text{ matrix,}\\\\\n",
    "  \\mathbf{f} & \\text{ is an }n_p\\text{-dimensional vector,}\\\\\n",
    "  \\mathbf{A} & \\text{ is an } n_c \\times n_p \\text{ matrix (}n_c = \\text{number of constraints),}\\\\\n",
    "  \\mathbf{b} & \\text{ is an }n_c\\text{-dimensional vector.}\n",
    "\\end{cases}\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Cost Function</center></h1>\n",
    "\n",
    "**Equation 5-13: Linear SVM classifier cost function**\n",
    "\n",
    "$\n",
    "J(\\mathbf{w}, b) = \\dfrac{1}{2} \\mathbf{w}^T \\mathbf{w} \\quad + \\quad C {\\displaystyle \\sum\\limits_{i=1}^{m}max\\left(0, t^{(i)} - (\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\right)}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Cost Function</center></h1>\n",
    "\n",
    "<center><img src=\"img/loss-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/hinge.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Optimization - Finding Global Optima</center></h1>\n",
    "\n",
    "<center><img src=\"img/optimization-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convex Function</center></h1>\n",
    "\n",
    "<center><img src=\"img/convex-function.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Convex Function Examples</center></h1>\n",
    "\n",
    "<center><img src=\"img/convex-examples.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM Cost Function Is Convex</center></h1>\n",
    "\n",
    "<center><img src=\"img/svm-convex.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Primal and Dual Problem</center></h1>\n",
    "\n",
    "<center><img src=\"img/dual-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Primal and Dual Problem Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/representer.png\" align=\"center\"/></center>\n",
    "\n",
    "<center><img src=\"img/dual-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Primal and Dual Problem Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/dual-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Primal and Dual Versions of SVM Classifier</center></h1>\n",
    "\n",
    "<center><img src=\"img/dual-4.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1><center>Dual Problem SVM</center></h1>\n",
    "\n",
    "<center><img src=\"img/dual-5.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Dual Form Optimization</center></h1>\n",
    "\n",
    "**Equation 5-6: Dual form of the linear SVM objective**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\underset{\\mathbf{\\alpha}}{\\operatorname{minimize}}\n",
    "\\dfrac{1}{2}\\sum\\limits_{i=1}^{m}{\n",
    "  \\sum\\limits_{j=1}^{m}{\n",
    "  \\alpha^{(i)} \\alpha^{(j)} t^{(i)} t^{(j)} {\\mathbf{x}^{(i)}}^T \\mathbf{x}^{(j)}\n",
    "  }\n",
    "} \\quad - \\quad \\sum\\limits_{i=1}^{m}{\\alpha^{(i)}}\\\\\n",
    "\\text{subject to}\\quad \\alpha^{(i)} \\ge 0 \\quad \\text{for }i = 1, 2, \\dots, m\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Equation 5-7: From the dual solution to the primal solution**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "&\\hat{\\mathbf{w}} = \\sum_{i=1}^{m}{\\hat{\\alpha}}^{(i)}t^{(i)}\\mathbf{x}^{(i)}\\\\\n",
    "&\\hat{b} = \\dfrac{1}{n_s}\\sum\\limits_{\\scriptstyle i=1 \\atop {\\scriptstyle {\\hat{\\alpha}}^{(i)} > 0}}^{m}{\\left(t^{(i)} - ({\\hat{\\mathbf{w}}}^T \\mathbf{x}^{(i)})\\right)}\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Non-Linearly Separable Data Revisited</center></h1>\n",
    "\n",
    "<center><img src=\"img/non-linear.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Transforming Non-Linear Data</center></h1>\n",
    "\n",
    "<center><img src=\"img/transform.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVM in Transformed Feature Space</center></h1>\n",
    "\n",
    "<center><img src=\"img/feature-map.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Image from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Equation 5-8: Second-degree polynomial mapping**\n",
    "\n",
    "$\n",
    "\\phi\\left(\\mathbf{x}\\right) = \\phi\\left( \\begin{pmatrix}\n",
    "  x_1 \\\\\n",
    "  x_2\n",
    "\\end{pmatrix} \\right) = \\begin{pmatrix}\n",
    "  {x_1}^2 \\\\\n",
    "  \\sqrt{2} \\, x_1 x_2 \\\\\n",
    "  {x_2}^2\n",
    "\\end{pmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Kernel Trick</center></h1>\n",
    "\n",
    "<center><img src=\"img/kernel.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Equation 5-9: Kernel trick for a 2^nd^-degree polynomial mapping**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\phi(\\mathbf{a})^T \\phi(\\mathbf{b}) & \\quad = \\begin{pmatrix}\n",
    "  {a_1}^2 \\\\\n",
    "  \\sqrt{2} \\, a_1 a_2 \\\\\n",
    "  {a_2}^2\n",
    "  \\end{pmatrix}^T \\begin{pmatrix}\n",
    "  {b_1}^2 \\\\\n",
    "  \\sqrt{2} \\, b_1 b_2 \\\\\n",
    "  {b_2}^2\n",
    "\\end{pmatrix} = {a_1}^2 {b_1}^2 + 2 a_1 b_1 a_2 b_2 + {a_2}^2 {b_2}^2 \\\\\n",
    " & \\quad = \\left( a_1 b_1 + a_2 b_2 \\right)^2 = \\left( \\begin{pmatrix}\n",
    "  a_1 \\\\\n",
    "  a_2\n",
    "\\end{pmatrix}^T \\begin{pmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2\n",
    "  \\end{pmatrix} \\right)^2 = (\\mathbf{a}^T \\mathbf{b})^2\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Kernels</center></h1>\n",
    "\n",
    "<center><img src=\"img/kernels.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size=1>Slide from: Ref. [3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Equation 5-10: Common kernels**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\text{Linear:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\mathbf{a}^T \\mathbf{b} \\\\\n",
    "\\text{Polynomial:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\left(\\gamma \\mathbf{a}^T \\mathbf{b} + r \\right)^d \\\\\n",
    "\\text{Gaussian RBF:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\exp({\\displaystyle -\\gamma \\left\\| \\mathbf{a} - \\mathbf{b} \\right\\|^2}) \\\\\n",
    "\\text{Sigmoid:} & \\quad K(\\mathbf{a}, \\mathbf{b}) = \\tanh\\left(\\gamma \\mathbf{a}^T \\mathbf{b} + r\\right)\n",
    "\\end{split}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Online SVM</center></h1>\n",
    "\n",
    "\n",
    "- For linear SVM classifiers, one method for implementing an online SVM classifier is to use Gradient Descent (e.g., using SGDClassifier ) to minimize the cost function in Equation 5-13, which is derived from the primal problem.\n",
    "\n",
    "**Equation 5-13: Linear SVM classifier cost function**\n",
    "\n",
    "$\n",
    "J(\\mathbf{w}, b) = \\dfrac{1}{2} \\mathbf{w}^T \\mathbf{w} \\quad + \\quad C {\\displaystyle \\sum\\limits_{i=1}^{m}max\\left(0, t^{(i)} - (\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\right)}\n",
    "$\n",
    "\n",
    "\n",
    "- Usually, Gradient Descent converges much more slowly than the methods based on Quadratic Programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Summary</center></h1>\n",
    "\n",
    "- Linear Classifiers and Perceptron Algorithm\n",
    "\n",
    "- Hard Margin vs Soft Margin SVM\n",
    "\n",
    "- Slack Variables\n",
    "\n",
    "- Hyperparameter C and Bias-Varince Trade-off\n",
    "\n",
    "- SVM Optimization Problem: Constrained and Unconstrained Form\n",
    "\n",
    "- SVM Cost Function and Convex Discussion\n",
    "\n",
    "- Hinge Loss Function\n",
    "\n",
    "- Primal and Dual Problem\n",
    "\n",
    "- Non-Linear SVM, Feature Map and Kernel Trick\n",
    "\n",
    "- Online SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>References</center></h1>\n",
    "\n",
    "[1] \"Hands-On ML\" Textbook 2nd Edition Oct 2019\n",
    "\n",
    "[2] [Zisserman's SVM Slides-1 - Oxford University](http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)\n",
    "\n",
    "[3] [Zisserman's SVM Slides-2 - Oxford University](http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf)\n",
    "\n",
    "[4] N. Leal et al \"MARINE VESSEL RECOGNITION BY ACOUSTIC SIGNATURE\" 2015\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <h1><center>CMSC 478: Introduction to Machine Learning</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Chapter 6 - Decision Trees</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Agenda</center></h1>\n",
    "\n",
    "- <b> Chapter 6: Decision Trees</b>\n",
    "- <b> Learning Objectives:</b>\n",
    "- What is a decision tree?\n",
    "- Gini and Entropy\n",
    "- CART Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>What is a Decision Tree?</center></h1>\n",
    "\n",
    "<center><img src=\"img/oz-tree.jpg\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Wizard of Oz (1939) https://www.imdb.com/title/tt0032138/ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>What is a Decision Tree?</center></h1>\n",
    "\n",
    "<center><img src=\"img/orange-lemon-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>From Zemel et al Slides (UOfT)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Decision Tree Prediction</center></h1>\n",
    "\n",
    "<center><img src=\"img/orange-lemon-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>From Zemel et al Slides (UOfT)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Decision Tree - Decision Boundary</center></h1>\n",
    "\n",
    "<center><img src=\"img/orange-lemon-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>From Zemel et al Slides (UOfT)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Decision Tree - Tax Fraud Detection</center></h1>\n",
    "\n",
    "<center><img src=\"img/tax-fraud-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>From Singh's Slides (CMU)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Decision Tree - Tax Fraud Detection</center></h1>\n",
    "\n",
    "<center><img src=\"img/tax-fraud-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>From Singh's Slides (CMU)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Decision Tree - Tax Fraud Detection</center></h1>\n",
    "\n",
    "<center><img src=\"img/tax-fraud-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>From Singh's Slides (CMU)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Decision Tree - Tax Fraud Detection</center></h1>\n",
    "\n",
    "<center><img src=\"img/tax-fraud-4.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>From Singh's Slides (CMU)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Decision Tree - Tax Fraud Detection</center></h1>\n",
    "\n",
    "<center><img src=\"img/tax-fraud-5.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>From Singh's Slides (CMU)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Decision Tree - Tax Fraud Detection</center></h1>\n",
    "\n",
    "<center><img src=\"img/tax-fraud-6.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>From Singh's Slides (CMU)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>So Far...</center></h1>\n",
    "\n",
    "- What does a decision tree represent.\n",
    "\n",
    "\n",
    "- Given a decision tree, how do we assign label to a test point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Now...</center></h1>\n",
    "\n",
    "- How do we learn a decision tree from training data?\n",
    "\n",
    "\n",
    "- Which feature is the \"best\" to split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Decision Tree - Play Tennis?</center></h1>\n",
    "\n",
    "<img src=\"img/play-tennis.png\" align=\"left\"/>\n",
    "\n",
    "<img src=\"img/play-tennis-dt-s.png\" align=\"middle\"/>\n",
    "\n",
    "<img src=\"img/play-tennis-data-s.png\" align=\"right\"/>\n",
    "\n",
    "<font size='1'>Image from Mausam's Slides (UW)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>The Dataset</center></h1>\n",
    "\n",
    "<center><img src=\"img/play-tennis-data.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Adapted from Carpuat's Slides (UMD)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>And The Tree! But How Is The Tree Built?</center></h1>\n",
    "\n",
    "<img src=\"img/play-tennis-dt.png\" align=\"center\"/>\n",
    "\n",
    "<font size='1'>Adapted from Carpuat's Slides (UMD)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Learning Decision Trees</center></h1>\n",
    "\n",
    "Learning the simplest (smallest) decision tree is an NP complete problem.\n",
    "\n",
    "\n",
    "Use a recursive and greedy heuristic.\n",
    "\n",
    "\n",
    "* Start from an empty decision tree.\n",
    "\n",
    "\n",
    "* Split on next best feature.\n",
    "\n",
    "\n",
    "* Recurse.\n",
    "\n",
    "\n",
    "What is the best feature?\n",
    "\n",
    "\n",
    "We use information theory to guide us for choosing the best split. <font color='blue'><b>Impurity: Gini & Entropy</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Choosing The Best Feature To Split: X1 or X2?</center></h1>\n",
    "\n",
    "<center><img src=\"img/best-split.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Adapted from Zemel et al Slides (UOfT)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Choosing The Best Feature To Split: Balance or Employment?</center></h1>\n",
    "\n",
    "<center><img src=\"img/information-gain.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Adapted from Pedro Domingos Slides</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Impurity</center></h1>\n",
    "\n",
    "<center><img src=\"img/impurity.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Adapted from Pedro Domingos Slides</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Gini Index</center></h1>\n",
    "\n",
    "**Equation 6-1: Gini impurity**\n",
    "\n",
    "$\n",
    "G_i = 1 - \\sum\\limits_{k=1}^{n}{{p_{i,k}}^2}\n",
    "$\n",
    "\n",
    "\n",
    "- $Gini_i$ Gini Index of node $i$\n",
    "\n",
    "- $n$ Number of classes.\n",
    "\n",
    "- $p_{i,k}$ is the ratio of class $k$ instances among the training instances in the $i$th node.\n",
    "\n",
    "- If perfectly classified by the split, Gini Index would be zero. \n",
    "\n",
    "- A feature with lower Gini index should be preferred.\n",
    "\n",
    "- sklearn by default uses `gini`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Entropy</center></h1>\n",
    "\n",
    "<center><img src=\"img/entropy-formula.png\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "- $C$ Number of classes.\n",
    "\n",
    "- $p_i$ Probability of class $i$\n",
    "\n",
    "- The lower the Entropy, the lower impurity. A smaller value of Entropy is better!\n",
    "\n",
    "<font size='1'>From: http://www.learnbymarketing.com/481/decision-tree-flavors-gini-info-gain/</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Equation 6-3: Entropy**\n",
    "\n",
    "$\n",
    "H_i = -\\sum\\limits_{k=1 \\atop p_{i,k} \\ne 0}^{n}{{p_{i,k}}\\log_2(p_{i,k})}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Using Gini Index in CART Algorithm</center></h1>\n",
    "\n",
    "`Use Gini Index:\n",
    "    for each branch in split:\n",
    "        Calculate percent branch represents #Used for weighting\n",
    "        for each class in branch:\n",
    "            Calculate probability of class in the given branch.\n",
    "            Square the class probability.\n",
    "        Sum the squared class probabilities.\n",
    "        Subtract the sum from 1. #This is the Ginin Index for branch\n",
    "    Weight each branch based on the baseline probability.\n",
    "    Sum the weighted gini index for each split.\n",
    "    Use the feature with the lowest Gini for next split.\n",
    "    Recurse.`\n",
    "    \n",
    "<font size='1'>Adapted from: http://www.learnbymarketing.com/481/decision-tree-flavors-gini-info-gain/ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Cost Function of CART Algorithm</center></h1>\n",
    "\n",
    "\n",
    "**Equation 6-2: CART cost function for classification**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "&J(k, t_k) = \\dfrac{m_{\\text{left}}}{m}G_\\text{left} + \\dfrac{m_{\\text{right}}}{m}G_{\\text{right}}\\\\\n",
    "&\\text{where }\\begin{cases}\n",
    "G_\\text{left/right} \\text{ measures the impurity of the left/right subset,}\\\\\n",
    "m_\\text{left/right} \\text{ is the number of instances in the left/right subset.}\n",
    "\\end{cases}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "<font size='1'>From Hands-On ML Textbook</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Computing the Cost Function for Tennis Player -1</center></h1>\n",
    "\n",
    "|  Outlook | Yes | No | Number of Instances |\n",
    "|:--------:|:---:|:--:|:-------------------:|\n",
    "|   Sunny  |  2  |  3 |          5          |\n",
    "| Overcast |  4  |  0 |          4          |\n",
    "|   Rain   |  3  |  2 |          5          |\n",
    "\n",
    "Gini(Outlook=Sunny) = $1 – (2/5)^2 – (3/5)^2 = 1 – 0.16 – 0.36 = 0.48$\n",
    "\n",
    "Gini(Outlook=Overcast) = $1 – (4/4)^2 – (0/4)^2 = 0$\n",
    "\n",
    "Gini(Outlook=Rain) = $1 – (3/5)^2 – (2/5)^2 = 1 – 0.36 – 0.16 = 0.48$\n",
    "\n",
    "Now, we compute the weighted average of all gini indexes for \"Outlook\" feature.\n",
    "\n",
    "Gini(Outlook) = $(5/14) * 0.48 + (4/14) * 0 + (5/14) * 0.48 = 0.171 + 0 + 0.171 = 0.342$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Computing the Cost Function for Tennis Player -2</center></h1>\n",
    "\n",
    "| Temperature | Yes | No | Number of Instances |\n",
    "|:-----------:|:---:|:--:|:-------------------:|\n",
    "|     Hot     |  2  |  2 |          4          |\n",
    "|     Cool    |  3  |  1 |          4          |\n",
    "|     Mild    |  4  |  2 |          6          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Gini(Temp=Hot) = $1 – (2/4)^2 – (2/4)^2 = 0.5$\n",
    "\n",
    "\n",
    "Gini(Temp=Cool) = $1 – (3/4)^2 – (1/4)^2 = 1 – 0.5625 – 0.0625 = 0.375$\n",
    "\n",
    "\n",
    "Gini(Temp=Mild) = $1 – (4/6)^2 – (2/6)^2 = 1 – 0.444 – 0.111 = 0.445$\n",
    "\n",
    "\n",
    "Gini(Temp) = $(4/14) * 0.5 + (4/14) * 0.375 + (6/14) * 0.445 = 0.142 + 0.107 + 0.190 = 0.439$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Computing The Cost Function for Tennis Player - Final Ranking</center></h1>\n",
    "\n",
    "|   Feature   | Gini Index |\n",
    "|:-----------:|:----------:|\n",
    "|   Outlook   |    0.342   |\n",
    "|   Humidity  |    0.367   |\n",
    "|     Wind    |    0.428   |\n",
    "| Temperature |    0.439   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Building The Decision Tree -1</center></h1>\n",
    "\n",
    "- After ranking features according to impurity, start with an empty tree and build it recursively, so the first node is the root to be selected for splitting.\n",
    "\n",
    "\n",
    "- Outlook is selected as the root node because it has the lowest impurity according to Gini.\n",
    "\n",
    "\n",
    "- The left and the right branches need further expansion.\n",
    "\n",
    "\n",
    "- The middle branch (Outlook=overcast) has Gini=0 and is pure so no further expansion is needed for the middle branch.\n",
    "\n",
    "<center><img src=\"img/dt-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from [Furnkranz's Slides](http://www.ke.tu-darmstadt.de/lehre/archiv/ws0809/mldm/dt.pdf)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Building The Decision Tree -2</center></h1>\n",
    "\n",
    "- Next, for the left branch, humidity is selected as it has the 2nd lowest impurity according to Gini.\n",
    "\n",
    "\n",
    "<center><img src=\"img/dt-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from [Furnkranz's Slides](http://www.ke.tu-darmstadt.de/lehre/archiv/ws0809/mldm/dt.pdf)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Building The Decision Tree -3</center></h1>\n",
    "\n",
    "- After splitting humidity for the left branch, no further expansion is needed, so the recursion backtracks to expand the right branch.\n",
    "\n",
    "\n",
    "- Windy is selected feature to split for the right branch because it has the 3rd lowest impurity according to Gini.\n",
    "\n",
    "<center><img src=\"img/dt-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from [Furnkranz's Slides](http://www.ke.tu-darmstadt.de/lehre/archiv/ws0809/mldm/dt.pdf)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Building The Decision Tree - Final Tree</center></h1>\n",
    "\n",
    "- Notice that in the tennis player data, Wind feature is represented by strong (true) and weak (false). \n",
    "\n",
    "<center><img src=\"img/dt-4.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from [Furnkranz's Slides](http://www.ke.tu-darmstadt.de/lehre/archiv/ws0809/mldm/dt.pdf)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Overfitting in Decision Trees</center></h1>\n",
    "\n",
    "<center><img src=\"img/dt-overfit.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Adapted from Pedro Domingos Slides</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Decision Trees in X-Box</center></h1>\n",
    "\n",
    "<center><img src=\"img/xbox-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Adapted from Zemel et al Slides (UOfT) - J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, A. Blake. Real-Time Human Pose</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Decision Trees in X-Box</center></h1>\n",
    "\n",
    "<center><img src=\"img/xbox-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Adapted from Zemel et al Slides (UOfT)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Next Chapter - Random Forests (Ensemble Methods)</center></h1>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
